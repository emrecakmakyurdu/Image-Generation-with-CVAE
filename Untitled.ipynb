{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2feb47-8fd8-4597-b07b-01f0023430a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network architecture and layer shapes:\n",
      "Conv2d:\n",
      "  Input shape: [torch.Size([32, 3, 128, 128])]\n",
      "  Output shape: torch.Size([32, 64, 64, 64])\n",
      "ReLU:\n",
      "  Input shape: [torch.Size([32, 64, 64, 64])]\n",
      "  Output shape: torch.Size([32, 64, 64, 64])\n",
      "Conv2d:\n",
      "  Input shape: [torch.Size([32, 64, 64, 64])]\n",
      "  Output shape: torch.Size([32, 128, 32, 32])\n",
      "ReLU:\n",
      "  Input shape: [torch.Size([32, 128, 32, 32])]\n",
      "  Output shape: torch.Size([32, 128, 32, 32])\n",
      "Flatten:\n",
      "  Input shape: [torch.Size([32, 128, 32, 32])]\n",
      "  Output shape: torch.Size([32, 131072])\n",
      "Linear:\n",
      "  Input shape: [torch.Size([32, 131840])]\n",
      "  Output shape: torch.Size([32, 256])\n",
      "Linear:\n",
      "  Input shape: [torch.Size([32, 131840])]\n",
      "  Output shape: torch.Size([32, 256])\n",
      "Linear:\n",
      "  Input shape: [torch.Size([32, 1024])]\n",
      "  Output shape: torch.Size([32, 131072])\n",
      "ConvTranspose2d:\n",
      "  Input shape: [torch.Size([32, 128, 32, 32])]\n",
      "  Output shape: torch.Size([32, 64, 64, 64])\n",
      "ReLU:\n",
      "  Input shape: [torch.Size([32, 64, 64, 64])]\n",
      "  Output shape: torch.Size([32, 64, 64, 64])\n",
      "ConvTranspose2d:\n",
      "  Input shape: [torch.Size([32, 64, 64, 64])]\n",
      "  Output shape: torch.Size([32, 32, 128, 128])\n",
      "ReLU:\n",
      "  Input shape: [torch.Size([32, 32, 128, 128])]\n",
      "  Output shape: torch.Size([32, 32, 128, 128])\n",
      "ConvTranspose2d:\n",
      "  Input shape: [torch.Size([32, 32, 128, 128])]\n",
      "  Output shape: torch.Size([32, 3, 128, 128])\n",
      "Sigmoid:\n",
      "  Input shape: [torch.Size([32, 3, 128, 128])]\n",
      "  Output shape: torch.Size([32, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # 128 -> 64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 64 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        flattened_dim = 128 * (image_dim // 4) * (image_dim // 4)  # 128 -> 32 (downsampled by 4)\n",
    "        self.fc_mu = nn.Linear(flattened_dim + text_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flattened_dim + text_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc = nn.Linear(latent_dim + text_dim, flattened_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 32 -> 64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # 64 -> 128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1),  # 128 -> 128\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        img_features = self.encoder(x)\n",
    "        if c.dim() == 1:\n",
    "            c = c.unsqueeze(0).repeat(x.size(0), 1)\n",
    "        combined = torch.cat([img_features, c], dim=1)\n",
    "        mu = self.fc_mu(combined)\n",
    "        logvar = self.fc_logvar(combined)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        if c.dim() == 1:\n",
    "            c = c.unsqueeze(0).repeat(z.size(0), 1)\n",
    "        combined = torch.cat([z, c], dim=1)\n",
    "        out = self.fc(combined).view(z.size(0), 128, 32, 32)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar\n",
    "\n",
    "\n",
    "def print_layer_shapes(model, input_tensors):\n",
    "    \"\"\"\n",
    "    Print input/output shapes of each layer using forward hooks.\n",
    "    \"\"\"\n",
    "    hooks = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        print(f\"{module.__class__.__name__}:\")\n",
    "        print(f\"  Input shape: {[i.shape for i in input]}\")\n",
    "        print(f\"  Output shape: {output.shape if isinstance(output, torch.Tensor) else [o.shape for o in output]}\")\n",
    "\n",
    "    # Register hooks for each layer\n",
    "    for name, layer in model.named_modules():\n",
    "        if len(list(layer.children())) == 0:  # Register only on leaf layers\n",
    "            hooks.append(layer.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Pass dummy inputs\n",
    "    with torch.no_grad():\n",
    "        model(*input_tensors)\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "image_dim = 128\n",
    "text_dim = 768\n",
    "latent_dim = 256\n",
    "cvae = CVAE(image_dim=image_dim, text_dim=text_dim, latent_dim=latent_dim)\n",
    "\n",
    "# Dummy inputs\n",
    "dummy_image = torch.randn(32, 3, image_dim, image_dim)  # Batch size of 2, 3-channel image\n",
    "dummy_text_embedding = torch.randn(32, text_dim)  # Batch size of 2, text embedding size\n",
    "\n",
    "# Print layer shapes\n",
    "print(\"Network architecture and layer shapes:\")\n",
    "print_layer_shapes(cvae, (dummy_image, dummy_text_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e319039f-2d79-40dc-9588-d9d7f2ab1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, captions_file, instances_file, transform, embedding_dir=None):\n",
    "        self.root = root\n",
    "        self.coco_captions = COCO(captions_file)\n",
    "        self.coco_instances = COCO(instances_file)\n",
    "        self.ids = list(self.coco_captions.imgToAnns.keys())\n",
    "        self.transform = transform\n",
    "        self.embedding_dir = embedding_dir\n",
    "\n",
    "        # Lazy initialization of tokenizer and model\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "        self.grouped_data = None  # Store grouped data after group_by_category is called\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "            self.model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(\"cuda\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.grouped_data:\n",
    "            return sum(len(samples) for samples in self.grouped_data.values())\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.grouped_data:\n",
    "            flattened_data = [\n",
    "                (category, img_id, captions)\n",
    "                for category, samples in self.grouped_data.items()\n",
    "                for img_id, captions in samples\n",
    "            ]\n",
    "            category, img_id, captions = flattened_data[idx]\n",
    "        else:\n",
    "            img_id = self.ids[idx]\n",
    "            ann = self.coco_captions.imgToAnns[img_id]\n",
    "            captions = [a['caption'] for a in ann]\n",
    "            category = None\n",
    "\n",
    "        img_info = self.coco_captions.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.embedding_dir:\n",
    "            embedding_path = os.path.join(self.embedding_dir, f\"{img_id}.pt\")\n",
    "            if not os.path.exists(embedding_path):\n",
    "                raise FileNotFoundError(f\"Embedding not found for image ID {img_id}\")\n",
    "            embeddings = torch.load(embedding_path)\n",
    "        else:\n",
    "            self._initialize_model()\n",
    "            embeddings = []\n",
    "            for caption in captions:\n",
    "                tokenized = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=16).to(\"cuda\")\n",
    "                with torch.no_grad():\n",
    "                    text_embedding = self.model(**tokenized).last_hidden_state.mean(dim=1).squeeze(0).cpu()\n",
    "                    embeddings.append(text_embedding)\n",
    "            embeddings = torch.stack(embeddings)\n",
    "\n",
    "        return image, embeddings, category\n",
    "\n",
    "    def group_by_category(self):\n",
    "        categories = self.coco_instances.loadCats(self.coco_instances.getCatIds())\n",
    "        category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "        grouped_data = {cat['name']: [] for cat in categories}\n",
    "\n",
    "        for img_id in self.ids:\n",
    "            ann_ids = self.coco_instances.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco_instances.loadAnns(ann_ids)\n",
    "            category_ids = {ann['category_id'] for ann in anns}\n",
    "            caption_ann_ids = self.coco_captions.getAnnIds(imgIds=img_id)\n",
    "            caption_anns = self.coco_captions.loadAnns(caption_ann_ids)\n",
    "            captions = [ann['caption'] for ann in caption_anns]\n",
    "\n",
    "            for category_id in category_ids:\n",
    "                category_name = category_id_to_name[category_id]\n",
    "                grouped_data[category_name].append((img_id, captions))\n",
    "\n",
    "        self.grouped_data = grouped_data\n",
    "        return grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db16efaf-72ce-4486-bf3a-327712231547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root, captions_file, instances_file, transform):\n",
    "        self.root = root\n",
    "        self.coco_captions = COCO(captions_file)\n",
    "        self.coco_instances = COCO(instances_file)  # Load instances file for categories\n",
    "        self.ids = list(self.coco_captions.imgToAnns.keys())\n",
    "        self.transform = transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        ann = self.coco_captions.imgToAnns[img_id]\n",
    "        caption = ann[0]['caption']  # Get the first annotation's caption\n",
    "\n",
    "        img_info = self.coco_captions.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize caption and get BERT embeddings\n",
    "        tokenized = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=16)\n",
    "        with torch.no_grad():\n",
    "            text_embedding = self.model(**tokenized).last_hidden_state.mean(dim=1).squeeze(0)\n",
    "\n",
    "        return image, text_embedding\n",
    "\n",
    "    def group_by_category(self):\n",
    "        \"\"\"\n",
    "        Group all samples by category and fetch corresponding captions.\n",
    "\n",
    "        :return: Dictionary with category names as keys and lists of (image_id, captions) as values.\n",
    "        \"\"\"\n",
    "        # Get all categories from instances\n",
    "        categories = self.coco_instances.loadCats(self.coco_instances.getCatIds())\n",
    "        category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "\n",
    "        # Initialize dictionary to group samples by category\n",
    "        grouped_data = {cat['name']: [] for cat in categories}\n",
    "        \n",
    "        # Iterate over all image IDs\n",
    "        for img_id in self.ids:\n",
    "            # Get annotations from instances file\n",
    "            ann_ids = self.coco_instances.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco_instances.loadAnns(ann_ids)\n",
    "\n",
    "            # Get the associated category IDs\n",
    "            category_ids = {ann['category_id'] for ann in anns}\n",
    "\n",
    "            # Fetch captions from captions file\n",
    "            caption_ann_ids = self.coco_captions.getAnnIds(imgIds=img_id)\n",
    "            caption_anns = self.coco_captions.loadAnns(caption_ann_ids)\n",
    "            captions = [ann['caption'] for ann in caption_anns]\n",
    "\n",
    "            # Group by category name\n",
    "            for category_id in category_ids:\n",
    "                category_name = category_id_to_name[category_id]\n",
    "                grouped_data[category_name].append((img_id, captions))\n",
    "\n",
    "        return grouped_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68d0f7ed-8238-4e69-983c-196e8e33283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, dataloader, lr=0.001, device=\"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(recon_x, x, mu, logvar):\n",
    "        recon_loss = nn.MSELoss()(recon_x, x)\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss, kl_loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()            \n",
    "            train_loss = 0\n",
    "            recon_loss_total = 0\n",
    "            kl_loss_total = 0\n",
    "            latent_vectors = []\n",
    "            labels = []  # Collect labels or categories for visualization\n",
    "            \n",
    "            for step, (images, text_embeddings) in enumerate(tqdm(self.dataloader)):\n",
    "                images = images.to(self.device)\n",
    "                text_embeddings = text_embeddings.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_images, mu, logvar = self.model(images, text_embeddings)\n",
    "                z = self.model.reparameterize(mu, logvar)  # Get latent vector\n",
    "                \n",
    "                # Collect latent vectors and labels\n",
    "                latent_vectors.append(z.detach().cpu())\n",
    "                labels.append(text_embeddings.detach().cpu())  # Replace with your label logic\n",
    "\n",
    "                recon_loss, kl_loss = self.loss_function(recon_images, images, mu, logvar)\n",
    "                loss = recon_loss + kl_loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                recon_loss_total += recon_loss.item()\n",
    "                kl_loss_total += kl_loss.item()\n",
    "\n",
    "                # Log training metrics to wandb every N steps\n",
    "                if step % 10 == 0:\n",
    "                    wandb.log({\n",
    "                        \"train/reconstruction_loss\": recon_loss.item(),\n",
    "                        \"train/kl_loss\": kl_loss.item(),\n",
    "                        \"train/total_loss\": loss.item(),\n",
    "                        \"train/step\": step + epoch * len(self.dataloader)\n",
    "                    })\n",
    "                    \n",
    "            torch.save(cvae.state_dict(), \"cvae_model1.pth\")\n",
    "            print(\"Model parameters saved successfully.\")\n",
    "            \n",
    "            # Concatenate collected latent vectors and labels\n",
    "            latent_vectors = torch.cat(latent_vectors, dim=0)\n",
    "            labels = torch.cat(labels, dim=0)  # Modify if you have actual labels\n",
    "            \n",
    "            # Visualize latent space after each epoch\n",
    "            self.visualize_latent_space(latent_vectors, labels, epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss / len(self.dataloader):.4f}\")\n",
    "            # Validation\n",
    "            \n",
    "            # Log generated image samples\n",
    "            if epoch % 1 == 0:  # Log every epoch\n",
    "                sample_image, sample_text = next(iter(self.val_dataloader))\n",
    "                generated_image = self.generate_sample(sample_image[0], sample_text[0])\n",
    "                wandb.log({\"generated_image\": wandb.Image(generated_image)})\n",
    "    \n",
    "    def generate_sample(self, image, text_embedding):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            recon_image, _, _ = self.model(image.unsqueeze(0).to(self.device), \n",
    "                                           text_embedding.unsqueeze(0).to(self.device))\n",
    "        return recon_image.squeeze(0).cpu()\n",
    "        \n",
    "    @staticmethod\n",
    "    def visualize_latent_space(latent_vectors, labels, epoch):\n",
    "        # Use PCA or t-SNE for dimensionality reduction\n",
    "        if latent_vectors.size(1) > 2:  # Dimensionality reduction if latent dim > 2\n",
    "            latent_vectors = TSNE(n_components=2).fit_transform(latent_vectors)\n",
    "        else:\n",
    "            latent_vectors = latent_vectors.numpy()\n",
    "\n",
    "        # Optional: Use t-SNE for nonlinear dimensionality reduction\n",
    "        # latent_vectors = TSNE(n_components=2).fit_transform(latent_vectors)\n",
    "\n",
    "        # Plot latent space\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        scatter = plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1], c=labels.numpy(), cmap='viridis', s=10)\n",
    "        plt.colorbar(scatter, label='Text Embedding Labels')  # Modify label as needed\n",
    "        plt.title(f\"Latent Space Visualization - Epoch {epoch + 1}\")\n",
    "        plt.xlabel(\"Latent Dim 1\")\n",
    "        plt.ylabel(\"Latent Dim 2\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"latent_space_epoch_{epoch + 1}.png\")  # Save plot for each epoch\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9909c31-bac8-4fa4-b8c3-c833ac3acf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model, device=\"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def generate_image(self, image, text_embedding):\n",
    "        self.model.eval()\n",
    "        image = image.to(self.device)\n",
    "        text_embedding = text_embedding.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.model.encode(image.unsqueeze(0), text_embedding.unsqueeze(0))\n",
    "            z = self.model.reparameterize(mu, logvar)\n",
    "            generated_image = self.model.decode(z, text_embedding.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        return generated_image\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_image(image_tensor):\n",
    "        plt.imshow(image_tensor.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50a9fb-b443-483b-80f2-cf496f5b6bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26c0f190-5f7e-4c01-ba0d-7c8bd06e6473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f2f48123c70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 788, in _clean_thread_parent_frames\n",
      "    if phase != \"start\":\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=21.79s)\n",
      "creating index...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 50\u001b[0m\n\u001b[1;32m     44\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     45\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m     46\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     47\u001b[0m ])\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(embedding_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(embedding_dir)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     dataset_for_precompute \u001b[38;5;241m=\u001b[39m \u001b[43mCocoDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstances_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputing embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m     precompute_embeddings_gpu(dataset_for_precompute, embedding_dir)\n",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m, in \u001b[0;36mCocoDataset.__init__\u001b[0;34m(self, root, captions_file, instances_file, transform, embedding_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m root\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_captions \u001b[38;5;241m=\u001b[39m COCO(captions_file)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_instances \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco_captions\u001b[38;5;241m.\u001b[39mimgToAnns\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py:86\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone (t=\u001b[39m\u001b[38;5;132;01m{:0.2f}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39m tic))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py:95\u001b[0m, in \u001b[0;36mCOCO.createIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 95\u001b[0m         \u001b[43mimgToAnns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mann\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m         anns[ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m ann\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#from coco_dataset import CocoDataset  # Ensure the updated CocoDataset is imported\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def precompute_embeddings_gpu(dataset, output_dir, batch_size=64):\n",
    "    \"\"\"\n",
    "    Precompute embeddings using GPU for faster computation.\n",
    "    \n",
    "    :param dataset: Dataset object\n",
    "    :param output_dir: Directory to save embeddings\n",
    "    :param batch_size: Number of captions processed in a batch\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    dataset._initialize_model()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset.model.to(device)  # Move model to GPU\n",
    "\n",
    "    for img_id in tqdm(dataset.ids, desc=\"Precomputing embeddings\"):\n",
    "        captions = [ann['caption'] for ann in dataset.coco_captions.imgToAnns[img_id]]\n",
    "\n",
    "        # Tokenize captions and move to GPU\n",
    "        tokenized = dataset.tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=16).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute embeddings on GPU\n",
    "            embeddings = dataset.model(**tokenized).last_hidden_state.mean(dim=1).cpu()  # Move back to CPU for saving\n",
    "\n",
    "        # Save embeddings\n",
    "        torch.save(embeddings, os.path.join(output_dir, f\"{img_id}.pt\"))\n",
    "\n",
    "\n",
    "root = \"./coco/images/train2017\"\n",
    "annotation_file = \"./coco/annotations/captions_train2017.json\"\n",
    "instances_file = \"./coco/annotations/instances_train2017.json\"\n",
    "embedding_dir = \"./precomputed_embeddings\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "if not os.path.exists(embedding_dir) or len(os.listdir(embedding_dir)) == 0:\n",
    "    dataset_for_precompute = CocoDataset(root, annotation_file, instances_file, transform=transform)\n",
    "    print(\"Precomputing embeddings...\")\n",
    "    precompute_embeddings_gpu(dataset_for_precompute, embedding_dir)\n",
    "\n",
    "dataset = CocoDataset(root, annotation_file, instances_file, transform=transform, embedding_dir=embedding_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for images, text_embeddings, category in dataloader:\n",
    "    print(\"Batch image shape:\", images.shape)\n",
    "    print(\"Batch text embedding shape:\", text_embeddings.shape)\n",
    "    print(\"Category:\", category)\n",
    "    break\n",
    "\n",
    "\n",
    "'''\n",
    "# Model setup\n",
    "image_dim = config.image_dim\n",
    "text_dim = config.text_dim\n",
    "latent_dim = config.latent_dim\n",
    "cvae = CVAE(image_dim=image_dim, text_dim=text_dim, latent_dim=latent_dim)\n",
    "\n",
    "# Training\n",
    "trainer = Trainer(model=cvae, dataloader=dataloader, lr=config.learning_rate, device=\"cuda\")\n",
    "trainer.train(epochs=config.epochs)\n",
    "\n",
    "torch.save(cvae.state_dict(), \"cvae_model.pth\")\n",
    "print(\"Model parameters saved successfully.\")\n",
    "\n",
    "# Inference\n",
    "inference = Inference(model=cvae, device=\"cuda\")\n",
    "sample_image, sample_text_embedding = dataset[0]\n",
    "generated_image = inference.generate_image(sample_image, sample_text_embedding)\n",
    "\n",
    "# Visualize\n",
    "inference.visualize_image(generated_image)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa04e3-668b-4393-bb31-cbdb3804303d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
